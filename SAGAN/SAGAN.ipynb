{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SAGAN.ipynb","provenance":[],"mount_file_id":"1IpreqDTGx_2QUN0TcJkcIn65LPvGaHag","authorship_tag":"ABX9TyMIrygooKnnwRkoX1gzLCAI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqMk4DdP855t","executionInfo":{"status":"ok","timestamp":1622635384384,"user_tz":-480,"elapsed":527,"user":{"displayName":"成漢恩","photoUrl":"","userId":"06396745810117539540"}},"outputId":"8b6efb65-cc3c-45df-b4b5-3b3857e7fe87"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Mpoj4vErYV0U","executionInfo":{"status":"ok","timestamp":1622637473466,"user_tz":-480,"elapsed":2039621,"user":{"displayName":"成漢恩","photoUrl":"","userId":"06396745810117539540"}},"outputId":"d434433b-319a-41db-ded9-0a08a2992f45"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# -------------------get_half_batch_ds-------------------\n","\n","def _process_x(x):\n","    return tf.expand_dims(tf.cast(x, tf.float32), axis=3) / 255. * 2 - 1\n","\n","def get_half_batch_ds(batch_size):\n","    return get_ds(batch_size//2)\n","\n","def get_ds(batch_size):\n","    (x, y), _ = keras.datasets.mnist.load_data()\n","    x = _process_x(x)\n","    y = tf.cast(y, tf.int32)\n","    ds = tf.data.Dataset.from_tensor_slices((x, y)).cache().shuffle(1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    return ds\n","\n","# -------------------------------------------------------\n","\n","class Attention(keras.layers.Layer):\n","    def __init__(self, gamma=0.01, trainable=True):\n","        super().__init__(trainable=trainable)\n","        self._gamma = gamma\n","        self.gamma = None\n","        self.f = None\n","        self.g = None\n","        self.h = None\n","        self.v = None\n","        self.attention = None\n","\n","    def build(self, input_shape):\n","        c = input_shape[-1]\n","        if c > 1:\n","          self.f = self.block(c//8)     # reduce channel size, reduce computation\n","          self.g = self.block(c//8)     # reduce channel size, reduce computation\n","          self.h = self.block(c//8)     # reduce channel size, reduce computation\n","        else:\n","          self.f = self.block(c)\n","          self.g = self.block(c)\n","          self.h = self.block(c)\n","        self.v = keras.layers.Conv2D(c, 1, 1)              # scale back to original channel size\n","        global GAMMA_id\n","        self.gamma = self.add_weight(\n","            \"gamma{}\".format(GAMMA_id), shape=None, initializer=keras.initializers.constant(self._gamma))\n","        # print(self.gamma.shape)\n","        GAMMA_id += 1\n","        print(GAMMA_id)\n","\n","    @staticmethod\n","    def block(c):\n","        return keras.Sequential([\n","            keras.layers.Conv2D(c, 1, 1),   # [n, w, h, c] 1*1conv\n","            keras.layers.Reshape((-1, c)),          # [n, w*h, c]\n","        ])\n","\n","    def call(self, inputs, **kwargs):\n","        f = self.f(inputs)    # [n, w, h, c] -> [n, w*h, c//8]\n","        g = self.g(inputs)    # [n, w, h, c] -> [n, w*h, c//8]\n","        h = self.h(inputs)    # [n, w, h, c] -> [n, w*h, c//8]\n","        s = tf.matmul(f, g, transpose_b=True)   # [n, w*h, c//8] @ [n, c//8, w*h] = [n, w*h, w*h]\n","        self.attention = tf.nn.softmax(s, axis=-1)\n","        context_wh = tf.matmul(self.attention, h)  # [n, w*h, w*h] @ [n, w*h, c//8] = [n, w*h, c//8]\n","        s = inputs.shape        # [n, w, h, c]\n","        cs = context_wh.shape   # [n, w*h, c//8]\n","        context = tf.reshape(context_wh, [-1, s[1], s[2], cs[-1]])    # [n, w, h, c//8]\n","        o = self.v(self.gamma * context) + inputs   # residual\n","        return o\n","\n","\n","class SAGAN(keras.Model):\n","    \"\"\"\n","    自注意力加强生成器能力,使用常用在SVM中的 hinge loss, 连续性loss.\n","    因为注意力的矩阵很大(w*h @ w*h), 所以训练起来比较慢, 意味着留有改动空间.\n","    里面的稳定W gradient的Spectral normalization（SN）写起来有点麻烦,\n","    我有空再考虑把这个 SN regularizer 写进来.\n","    \"\"\"\n","    def __init__(self, latent_dim, img_shape, gamma):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.img_shape = img_shape\n","        self.latent_dim = latent_dim\n","        self.g = self._get_generator()\n","        self.d = self._get_discriminator()\n","        self.opt = keras.optimizers.Adam(0.0002, beta_1=0.5)\n","        self.loss_func = keras.losses.Hinge()       # change loss to hinge based on the paper\n","\n","    def call(self, n, training=None, mask=None):\n","        return self.g.call(tf.random.normal((n, self.latent_dim)), training=training)\n","\n","    def _get_discriminator(self):\n","        model = keras.Sequential([\n","            keras.layers.GaussianNoise(0.01, input_shape=self.img_shape),\n","            keras.layers.Conv2D(16, 4, strides=2, padding='same'),\n","            keras.layers.BatchNormalization(),\n","            keras.layers.LeakyReLU(),\n","            Attention(self.gamma),\n","            keras.layers.Dropout(0.3),\n","\n","            keras.layers.Conv2D(32, 4, strides=2, padding='same'),\n","            keras.layers.BatchNormalization(),\n","            keras.layers.LeakyReLU(),\n","            keras.layers.Dropout(0.3),\n","\n","            keras.layers.Flatten(),\n","            keras.layers.Dense(1),\n","        ], name=\"discriminator\")\n","        model.summary()\n","        return model\n","\n","    def _get_generator(self):\n","        model = keras.Sequential([\n","            # [n, latent] -> [n, 7 * 7 * 128] -> [n, 7, 7, 128]\n","            keras.layers.Dense(7 * 7 * 128, input_shape=(self.latent_dim,)),\n","            keras.layers.BatchNormalization(),\n","            keras.layers.ReLU(),\n","            keras.layers.Reshape((7, 7, 128)),\n","\n","            # -> [n, 14, 14, 64]\n","            keras.layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'),\n","            keras.layers.BatchNormalization(),\n","            keras.layers.ReLU(),\n","            Attention(self.gamma),\n","\n","            # -> [n, 28, 28, 32]\n","            keras.layers.Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same'),\n","            keras.layers.BatchNormalization(),\n","            keras.layers.ReLU(),\n","            # -> [n, 28, 28, 1]\n","            keras.layers.Conv2D(1, (4, 4), padding='same', activation=keras.activations.tanh),\n","            Attention(self.gamma)\n","        ], name=\"generator\")\n","        model.summary()\n","        return model\n","\n","    def train_d(self, img, d_label):\n","        with tf.GradientTape() as tape:\n","            pred = self.d.call(img, training=True)\n","            loss = self.loss_func(d_label, pred)\n","        grads = tape.gradient(loss, self.d.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.d.trainable_variables))\n","        return loss\n","\n","    def train_g(self, d_label):\n","        with tf.GradientTape() as tape:\n","            g_img = self.call(len(d_label), training=True)\n","            pred = self.d.call(g_img, training=False)\n","            loss = self.loss_func(d_label, pred)\n","        grads = tape.gradient(loss, self.g.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.g.trainable_variables))\n","        return loss, g_img\n","\n","    def step(self, img):\n","        d_label = 2*tf.ones((len(img) * 2, 1), tf.float32)  # a stronger positive label?\n","        g_loss, g_img = self.train_g(d_label)\n","\n","        d_label = tf.concat((tf.ones((len(img), 1), tf.float32), -tf.ones((len(g_img)//2, 1), tf.float32)), axis=0)\n","        img = tf.concat((img, g_img[:len(g_img)//2]), axis=0)\n","        d_loss = self.train_d(img, d_label)\n","        return d_loss, g_loss\n","\n","\n","def train(gan, ds, epoch):\n","    t0 = time.time()\n","    a = 0\n","    for ep in range(epoch):\n","        for t, (img, _) in enumerate(ds):\n","            d_loss, g_loss = gan.step(img)\n","            if a == 0:\n","              t_img = img\n","              a = 1\n","            if t % 400 == 0:\n","                t1 = time.time()\n","                print(\n","                    \"ep={} | time={:.1f} | t={} | d_loss={:.2f} | g_loss={:.2f}\".format(\n","                        ep, t1 - t0, t, d_loss.numpy(), g_loss.numpy(), ))\n","                t0 = t1\n","        save_gan(gan, ep, t_img)\n","\n","\n","def save_gan(model, a, img, **kwargs):\n","    imgs = model.call(100, training=False).numpy()\n","    _save_gan(imgs, a, show_label=False)\n","    imgs = model.g.layers[-1](img)\n","    _save_gan2(imgs, a, show_label=False)\n","\n","def _save_gan(imgs, a, show_label=False, nc=5, nr=5):\n","    if not isinstance(imgs, np.ndarray):\n","        imgs = imgs.numpy()\n","    if imgs.ndim > 3:\n","        imgs = np.squeeze(imgs, axis=-1)\n","    plt.clf()\n","    plt.figure(0, (nc * 2, nr * 2))\n","    for c in range(nc):\n","        for r in range(nr):\n","            i = r * nc + c\n","            plt.subplot(nr, nc, i + 1)\n","            plt.imshow(imgs[i], cmap=\"gray_r\")\n","            plt.axis(\"off\")\n","    plt.savefig('/content/drive/MyDrive/張雲南/SAGAN/img/' + f'1_{a}.png')\n","    plt.close()\n","\n","def _save_gan2(imgs, a, show_label=False, nc=5, nr=5):\n","    if not isinstance(imgs, np.ndarray):\n","        imgs = imgs.numpy()\n","    if imgs.ndim > 3:\n","        imgs = np.squeeze(imgs, axis=-1)\n","    plt.clf()\n","    plt.figure(0, (nc * 2, nr * 2))\n","    for c in range(nc):\n","        for r in range(nr):\n","            i = r * nc + c\n","            plt.subplot(nr, nc, i + 1)\n","            plt.imshow(imgs[i], cmap=\"gray_r\")\n","            plt.axis(\"off\")\n","    plt.savefig('/content/drive/MyDrive/張雲南/SAGAN/img/' + f'2_{a}.png')\n","    plt.close()\n","\n","if __name__ == \"__main__\":\n","    GAMMA_id = 0\n","    LATENT_DIM = 100\n","    IMG_SHAPE = (28, 28, 1)\n","    BATCH_SIZE = 64\n","    GAMMA = 0.01\n","    EPOCH = 20\n","\n","    d = get_half_batch_ds(BATCH_SIZE)\n","    m = SAGAN(LATENT_DIM, IMG_SHAPE, GAMMA)\n","    train(m, d, EPOCH)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["1\n","2\n","Model: \"generator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 6272)              633472    \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 6272)              25088     \n","_________________________________________________________________\n","re_lu_3 (ReLU)               (None, 6272)              0         \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 14, 14, 64)        131136    \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n","_________________________________________________________________\n","re_lu_4 (ReLU)               (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","attention_2 (Attention)      (None, 14, 14, 64)        2137      \n","_________________________________________________________________\n","conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        32800     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 28, 28, 32)        128       \n","_________________________________________________________________\n","re_lu_5 (ReLU)               (None, 28, 28, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 28, 28, 1)         513       \n","_________________________________________________________________\n","attention_3 (Attention)      (None, 28, 28, 1)         9         \n","=================================================================\n","Total params: 825,539\n","Trainable params: 812,803\n","Non-trainable params: 12,736\n","_________________________________________________________________\n","3\n","Model: \"discriminator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gaussian_noise (GaussianNois (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 14, 14, 16)        272       \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 14, 14, 16)        64        \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","attention_4 (Attention)      (None, 14, 14, 16)        151       \n","_________________________________________________________________\n","dropout (Dropout)            (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 7, 7, 32)          8224      \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 7, 7, 32)          128       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 32)          0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 7, 7, 32)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1568)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 1569      \n","=================================================================\n","Total params: 10,408\n","Trainable params: 10,312\n","Non-trainable params: 96\n","_________________________________________________________________\n","ep=0 | time=29.6 | t=0 | d_loss=0.97 | g_loss=0.99\n","ep=0 | time=21.0 | t=400 | d_loss=0.36 | g_loss=2.75\n","ep=0 | time=21.1 | t=800 | d_loss=0.89 | g_loss=1.69\n","ep=0 | time=21.2 | t=1200 | d_loss=0.89 | g_loss=1.02\n","ep=0 | time=21.2 | t=1600 | d_loss=0.99 | g_loss=1.29\n","ep=1 | time=18.2 | t=0 | d_loss=0.97 | g_loss=0.51\n","ep=1 | time=21.0 | t=400 | d_loss=0.95 | g_loss=0.39\n","ep=1 | time=21.1 | t=800 | d_loss=1.05 | g_loss=1.41\n","ep=1 | time=21.0 | t=1200 | d_loss=0.99 | g_loss=1.85\n","ep=1 | time=21.0 | t=1600 | d_loss=1.02 | g_loss=2.00\n","ep=2 | time=16.8 | t=0 | d_loss=0.99 | g_loss=1.01\n","ep=2 | time=20.9 | t=400 | d_loss=0.95 | g_loss=1.29\n","ep=2 | time=21.1 | t=800 | d_loss=1.03 | g_loss=1.02\n","ep=2 | time=20.9 | t=1200 | d_loss=1.04 | g_loss=0.56\n","ep=2 | time=21.2 | t=1600 | d_loss=1.03 | g_loss=0.77\n","ep=3 | time=16.8 | t=0 | d_loss=0.91 | g_loss=1.15\n","ep=3 | time=20.9 | t=400 | d_loss=0.95 | g_loss=2.08\n","ep=3 | time=20.8 | t=800 | d_loss=0.93 | g_loss=2.61\n","ep=3 | time=21.0 | t=1200 | d_loss=1.01 | g_loss=0.48\n","ep=3 | time=20.9 | t=1600 | d_loss=1.01 | g_loss=0.84\n","ep=4 | time=16.4 | t=0 | d_loss=1.01 | g_loss=1.97\n","ep=4 | time=21.1 | t=400 | d_loss=0.97 | g_loss=2.15\n","ep=4 | time=20.9 | t=800 | d_loss=1.00 | g_loss=0.83\n","ep=4 | time=20.9 | t=1200 | d_loss=1.02 | g_loss=0.41\n","ep=4 | time=21.0 | t=1600 | d_loss=0.98 | g_loss=0.07\n","ep=5 | time=16.8 | t=0 | d_loss=0.99 | g_loss=2.09\n","ep=5 | time=21.0 | t=400 | d_loss=1.04 | g_loss=1.84\n","ep=5 | time=21.0 | t=800 | d_loss=1.04 | g_loss=1.76\n","ep=5 | time=20.9 | t=1200 | d_loss=0.84 | g_loss=1.48\n","ep=5 | time=21.1 | t=1600 | d_loss=1.03 | g_loss=1.21\n","ep=6 | time=16.3 | t=0 | d_loss=1.01 | g_loss=0.96\n","ep=6 | time=20.9 | t=400 | d_loss=0.84 | g_loss=0.50\n","ep=6 | time=20.8 | t=800 | d_loss=1.03 | g_loss=1.10\n","ep=6 | time=21.0 | t=1200 | d_loss=1.06 | g_loss=0.47\n","ep=6 | time=20.9 | t=1600 | d_loss=0.99 | g_loss=2.51\n","ep=7 | time=16.7 | t=0 | d_loss=1.00 | g_loss=0.83\n","ep=7 | time=21.0 | t=400 | d_loss=1.10 | g_loss=1.36\n","ep=7 | time=21.0 | t=800 | d_loss=1.09 | g_loss=1.60\n","ep=7 | time=21.0 | t=1200 | d_loss=0.97 | g_loss=2.73\n","ep=7 | time=20.9 | t=1600 | d_loss=0.97 | g_loss=1.36\n","ep=8 | time=16.4 | t=0 | d_loss=1.00 | g_loss=1.01\n","ep=8 | time=21.1 | t=400 | d_loss=0.93 | g_loss=2.27\n","ep=8 | time=20.8 | t=800 | d_loss=0.94 | g_loss=1.77\n","ep=8 | time=21.1 | t=1200 | d_loss=1.02 | g_loss=1.15\n","ep=8 | time=21.1 | t=1600 | d_loss=1.00 | g_loss=1.28\n","ep=9 | time=16.4 | t=0 | d_loss=1.03 | g_loss=1.13\n","ep=9 | time=20.9 | t=400 | d_loss=1.05 | g_loss=1.18\n","ep=9 | time=20.8 | t=800 | d_loss=1.00 | g_loss=1.23\n","ep=9 | time=20.9 | t=1200 | d_loss=0.98 | g_loss=1.81\n","ep=9 | time=21.0 | t=1600 | d_loss=1.04 | g_loss=0.81\n","ep=10 | time=16.7 | t=0 | d_loss=0.99 | g_loss=1.15\n","ep=10 | time=20.9 | t=400 | d_loss=0.95 | g_loss=0.27\n","ep=10 | time=21.0 | t=800 | d_loss=0.95 | g_loss=2.84\n","ep=10 | time=21.0 | t=1200 | d_loss=0.91 | g_loss=0.37\n","ep=10 | time=21.1 | t=1600 | d_loss=0.99 | g_loss=1.13\n","ep=11 | time=16.3 | t=0 | d_loss=1.02 | g_loss=1.02\n","ep=11 | time=20.9 | t=400 | d_loss=0.81 | g_loss=1.12\n","ep=11 | time=21.0 | t=800 | d_loss=0.99 | g_loss=1.35\n","ep=11 | time=21.0 | t=1200 | d_loss=1.05 | g_loss=2.21\n","ep=11 | time=20.9 | t=1600 | d_loss=0.98 | g_loss=0.82\n","ep=12 | time=16.4 | t=0 | d_loss=0.96 | g_loss=1.61\n","ep=12 | time=21.0 | t=400 | d_loss=1.11 | g_loss=0.79\n","ep=12 | time=20.8 | t=800 | d_loss=1.00 | g_loss=0.59\n","ep=12 | time=20.9 | t=1200 | d_loss=0.85 | g_loss=0.36\n","ep=12 | time=20.9 | t=1600 | d_loss=0.98 | g_loss=0.18\n","ep=13 | time=17.0 | t=0 | d_loss=0.93 | g_loss=0.28\n","ep=13 | time=21.1 | t=400 | d_loss=0.87 | g_loss=0.41\n","ep=13 | time=21.1 | t=800 | d_loss=0.84 | g_loss=2.25\n","ep=13 | time=20.8 | t=1200 | d_loss=0.88 | g_loss=1.23\n","ep=13 | time=21.1 | t=1600 | d_loss=0.97 | g_loss=2.20\n","ep=14 | time=16.3 | t=0 | d_loss=1.00 | g_loss=0.86\n","ep=14 | time=21.0 | t=400 | d_loss=0.92 | g_loss=0.85\n","ep=14 | time=21.0 | t=800 | d_loss=0.87 | g_loss=1.79\n","ep=14 | time=21.1 | t=1200 | d_loss=1.18 | g_loss=0.77\n","ep=14 | time=21.0 | t=1600 | d_loss=0.82 | g_loss=1.25\n","ep=15 | time=16.4 | t=0 | d_loss=1.05 | g_loss=0.88\n","ep=15 | time=20.8 | t=400 | d_loss=1.13 | g_loss=1.26\n","ep=15 | time=20.9 | t=800 | d_loss=1.31 | g_loss=1.59\n","ep=15 | time=20.8 | t=1200 | d_loss=0.78 | g_loss=1.61\n","ep=15 | time=20.8 | t=1600 | d_loss=1.12 | g_loss=0.70\n","ep=16 | time=16.3 | t=0 | d_loss=0.91 | g_loss=1.28\n","ep=16 | time=20.9 | t=400 | d_loss=0.92 | g_loss=1.85\n","ep=16 | time=20.8 | t=800 | d_loss=0.89 | g_loss=1.85\n","ep=16 | time=20.7 | t=1200 | d_loss=1.07 | g_loss=1.08\n","ep=16 | time=21.0 | t=1600 | d_loss=1.20 | g_loss=0.67\n","ep=17 | time=17.1 | t=0 | d_loss=1.11 | g_loss=1.72\n","ep=17 | time=21.0 | t=400 | d_loss=0.73 | g_loss=2.07\n","ep=17 | time=20.9 | t=800 | d_loss=0.88 | g_loss=2.17\n","ep=17 | time=20.9 | t=1200 | d_loss=0.79 | g_loss=2.00\n","ep=17 | time=21.1 | t=1600 | d_loss=0.50 | g_loss=1.86\n","ep=18 | time=16.4 | t=0 | d_loss=1.01 | g_loss=1.02\n","ep=18 | time=20.9 | t=400 | d_loss=0.98 | g_loss=2.54\n","ep=18 | time=21.0 | t=800 | d_loss=0.73 | g_loss=1.46\n","ep=18 | time=21.0 | t=1200 | d_loss=1.16 | g_loss=1.44\n","ep=18 | time=20.9 | t=1600 | d_loss=1.04 | g_loss=2.11\n","ep=19 | time=16.3 | t=0 | d_loss=0.91 | g_loss=0.06\n","ep=19 | time=20.9 | t=400 | d_loss=0.96 | g_loss=1.32\n","ep=19 | time=20.9 | t=800 | d_loss=1.03 | g_loss=0.98\n","ep=19 | time=20.9 | t=1200 | d_loss=0.95 | g_loss=1.18\n","ep=19 | time=20.9 | t=1600 | d_loss=0.88 | g_loss=2.56\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]}]}