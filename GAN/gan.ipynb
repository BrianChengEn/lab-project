{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gan.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPzY213a9Uo8hl5yRNHO8oR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"19i56-qpbkmAUDywPDKygyYs5k2xOB_t4"},"id":"F6Q-VyB83d98","executionInfo":{"status":"ok","timestamp":1609655348789,"user_tz":-480,"elapsed":527128,"user":{"displayName":"Brian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEG5ovklIn42vlKVDQCGtsky8uyFbZagMJgVSg2A=s64","userId":"12158858202818726983"}},"outputId":"1f12d664-0546-4f89-f61c-947074bef079"},"source":["from numpy import hstack\n","from numpy import zeros\n","from numpy import ones\n","import numpy as np\n","from numpy.random import rand\n","from numpy.random import randn\n","from keras.datasets import fashion_mnist, mnist\n","from keras.optimizers import Adam\n","from keras.losses import mse, binary_crossentropy\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.layers import *\n","from numpy.random import randint\n","from matplotlib import pyplot as plt\n","import os\n","\n","def define_generator(latent_dim):\n","    inputs = Input(shape=(latent_dim,))\n","    x = Dense(256)(inputs)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    x = Dense(512)(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    x = Dense(1024)(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    outputs = Dense(784, activation='tanh')(x)\n","    model = Model(inputs, outputs)\n","    return model\n","\n","def define_discriminator():\n","    inputs = Input(shape=(784,))\n","    x = Dense(1024)(inputs)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)\n","    x = Dense(512)(inputs)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)\n","    x = Dense(256)(inputs)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(0.3)(x)\n","    outputs = Dense(1, activation='sigmoid')(x)\n","    model = Model(inputs, outputs)\n","    opt = Adam(0.001, 0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    return model\n","\n","def define_gan(generator, discriminator):\n","    discriminator.trainable = False\n","    gen_noise = generator.input\n","    gen_output = generator.output\n","    gan_output = discriminator(gen_output)\n","    model = Model(gen_noise, gan_output)\n","    opt = Adam(0.001, 0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    return model\n","\n","def load_real_samples():\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n","    x_test = (x_train.astype(np.float32) - 127.5)/127.5\n","    x_train = x_train.reshape(60000, 784)\n","    x_test = x_train.reshape(60000, 784)\n","    return x_train, y_train, x_test, y_test\n","\n","def generate_real_samples(dataset, n):\n","    images = dataset\n","    ix = randint(0, images.shape[0], n)\n","    x = images[ix]\n","    y = ones((n, 1))\n","    return x, y\n","\n","def generate_latent_points(latent_dim, n):\n","    x_input = randn(latent_dim * n)\n","    x_input = x_input.reshape(n, latent_dim)\n","    return x_input\n","\n","def generate_fake_samples(generator, latent_dim, n):\n","    x_input = generate_latent_points(latent_dim, n)\n","    X = generator.predict(x_input)\n","    y = zeros((n, 1))\n","    return X, y\n","\n","def plot_results(*args,\n","                 batch_size=128,\n","                 model_name=\"vae_mnist\"):\n","\n","    encoder, decoder, x_test, y_test = args\n","    os.makedirs(model_name, exist_ok=True)\n","\n","    filename = os.path.join(model_name, \"vae_mean.png\")\n","    z_mean, _, _ = encoder.predict(x_test,\n","                                   batch_size=batch_size)\n","    plt.figure(figsize=(12, 10))\n","    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n","    plt.colorbar()\n","    plt.xlabel(\"Dimension 1\")\n","    plt.ylabel(\"Dimension 2\")\n","    plt.savefig(filename)\n","\n","    filename = os.path.join(model_name, \"digits_over_latent.png\")\n","    n = 30\n","    digit_size = 28\n","    figure = np.zeros((digit_size * n, digit_size * n))\n","    grid_x = np.linspace(-4, 4, n)\n","    grid_y = np.linspace(-4, 4, n)[::-1]\n","\n","    for i, yi in enumerate(grid_y):\n","        for j, xi in enumerate(grid_x):\n","            z_sample = np.array([[xi, yi]])\n","            x_decoded = decoder.predict(z_sample)\n","            digit = x_decoded[0].reshape(digit_size, digit_size)\n","            figure[i * digit_size: (i + 1) * digit_size,\n","                   j * digit_size: (j + 1) * digit_size] = digit\n","\n","    plt.figure(figsize=(10, 10))\n","    start_range = digit_size // 2\n","    end_range = n * digit_size + start_range + 1\n","    pixel_range = np.arange(start_range, end_range, digit_size)\n","    sample_range_x = np.round(grid_x, 1)\n","    sample_range_y = np.round(grid_y, 1)\n","    plt.xticks(pixel_range, sample_range_x)\n","    plt.yticks(pixel_range, sample_range_y)\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.imshow(figure, cmap='Greys_r')\n","    plt.savefig(filename)\n","\n","def sample_images(g_model, latent_dim):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, latent_dim))\n","        gen_imgs = g_model.predict(noise)\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs.reshape(-1, 28, 28)[cnt, :,:], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        plt.show()\n","    \n","def train(g_model, d_model, gan_model, latent_dim, dataset, n_epochs=10000, n_batch=128):\n","  half_batch = int(n_batch / 2)\n","  three_batch = int(n_batch / 3)\n","  \n","  x_train, _, x_test, y_test = load_real_samples()\n","  for i in range(n_epochs):\n","    x_real, y_real = generate_real_samples(dataset, half_batch)\n","    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","    d_loss1 = d_model.train_on_batch(x_real, y_real)\n","    d_loss2 = d_model.train_on_batch(x_fake, y_fake)\n","    x_gan = generate_latent_points(latent_dim, n_batch)\n","    y_gan = ones((n_batch, 1))\n","    g_loss1 = gan_model.train_on_batch(x_gan, y_gan)\n","    print('>%d, d1=%.3f, d2=%.3f, g1=%.3f' %\n","        (i, d_loss1, d_loss2, g_loss1))\n","    if i % 200 == 0:\n","      sample_images(g_model, latent_dim)\n","      \n","  g_model.save('cgan_generator.h5')\n","\n","  return g_model\n","            \n","latent_dim = 100\n","discriminator = define_discriminator()\n","generator = define_generator(latent_dim)\n","gan_model = define_gan(generator, discriminator)\n","dataset, _, x_test, y_test = load_real_samples()\n","train(generator, discriminator, gan_model, latent_dim, dataset)"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"GTzM-YIQ5vBF"},"source":[""],"execution_count":null,"outputs":[]}]}