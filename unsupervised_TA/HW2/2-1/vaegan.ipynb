{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from numpy.random import randint\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1] \n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def define_encoder(latent_dim):\n",
    "    inputs = Input(shape=(784,))\n",
    "    x = Dense(256)(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    \n",
    "    z_mean = Dense(latent_dim)(x)\n",
    "    z_log_var = Dense(latent_dim)(x)\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "    model = Model(inputs, [z_mean, z_log_var, z])\n",
    "    return model\n",
    "\n",
    "def define_generator(latent_dim):\n",
    "    inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(64)(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    outputs = Dense(784, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def define_discriminator():\n",
    "    inputs = Input(shape=(784,))\n",
    "    x = Dense(256)(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dense(128)(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dense(64)(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gen_noise = generator.input\n",
    "    gen_output = generator.output\n",
    "    gan_output = discriminator(gen_output)\n",
    "    model = Model(gen_noise, gan_output)\n",
    "    opt = Adam(lr=0.0008, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def load_real_samples():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def generate_real_samples(dataset, n):\n",
    "    images = dataset\n",
    "    ix = randint(0, images.shape[0], n)\n",
    "    x = images[ix]\n",
    "    y = ones((n, 1))\n",
    "    return x, y\n",
    "\n",
    "def generate_latent_points(latent_dim, n):\n",
    "    x_input = randn(latent_dim * n)\n",
    "    x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    X = generator.predict(x_input)\n",
    "    y = zeros((n, 1))\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(*args,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "\n",
    "    encoder, decoder, x_test, y_test = args\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "def sample_images(g_model, latent_dim):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "        gen_imgs = g_model.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs.reshape(-1, 28, 28)[cnt, :,:], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheng/opt/anaconda3/envs/p37/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output model_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_3.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "def train(e_model, g_model, d_model, gan_model, latent_dim, dataset, n_epochs=200, n_batch=128):\n",
    "    half_batch = int(n_batch / 3)\n",
    "    three_batch = int(n_batch / 2)\n",
    "\n",
    "    # VAE\n",
    "    inputs = Input(shape=(784,))\n",
    "\n",
    "    E_mean, E_log_var, Z = e_model(inputs)\n",
    "\n",
    "    outputs = g_model(Z)\n",
    "\n",
    "    vae = Model(inputs, outputs)\n",
    "    reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "    reconstruction_loss *= 784\n",
    "    kl_loss = 1 + E_log_var - K.square(E_mean) - K.exp(E_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    opt = Adam(0.001, 0.5)\n",
    "    vae.compile(optimizer=opt)\n",
    "\n",
    "    x_train, _, x_test, y_test = load_real_samples()\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(6):\n",
    "            x_train, _ = generate_real_samples(dataset, n_batch)\n",
    "            vae_loss = vae.train_on_batch(x_train, None)\n",
    "        for j in range(4):\n",
    "            x_real, y_real = generate_real_samples(dataset, three_batch)\n",
    "            x_fake, y_fake = generate_fake_samples(g_model, latent_dim, three_batch)\n",
    "            x_recon = vae.predict(x_real)\n",
    "            d_loss1 = d_model.train_on_batch(x_real, y_real)\n",
    "            d_loss2 = d_model.train_on_batch(x_fake, y_fake)\n",
    "            d_loss3 = d_model.train_on_batch(x_recon, y_fake)\n",
    "        for j in range(1):\n",
    "            x_gan = generate_latent_points(latent_dim, n_batch*2)\n",
    "            y_gan = ones((n_batch*2, 1))\n",
    "            g_loss1 = gan_model.train_on_batch(x_gan, y_gan)\n",
    "            x_real, y_real = generate_real_samples(dataset, n_batch*2)\n",
    "            x_code = e_model(x_real)\n",
    "            g_loss2 = gan_model.train_on_batch(x_code, y_gan)\n",
    "        print('>%d, d1=%.3f, d2=%.3f, d3=%.3f, vae=%.3f, g1=%.3f, g2=%.3f' %\n",
    "            (i, d_loss1, d_loss2, d_loss3, vae_loss, g_loss1, g_loss2))\n",
    "        if i % 10 == 0:\n",
    "            sample_images(g_model, latent_dim)\n",
    "            decoded_imgs = vae.predict(x_test)\n",
    "            n = 10  # How many digits we will display\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            for i in range(n):\n",
    "                # Display original\n",
    "                ax = plt.subplot(2, n, i + 1)\n",
    "                plt.imshow(x_test[i].reshape(28, 28))\n",
    "                plt.gray()\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "\n",
    "                # Display reconstruction\n",
    "                ax = plt.subplot(2, n, i + 1 + n)\n",
    "                plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "                plt.gray()\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            plt.show()\n",
    "    g_model.save('cgan_generator.h5')\n",
    "\n",
    "    plot_results(e_model, g_model, x_test, y_test)\n",
    "    return g_model\n",
    "            \n",
    "latent_dim = 2\n",
    "encoder = define_encoder(latent_dim)\n",
    "discriminator = define_discriminator()\n",
    "generator = define_generator(latent_dim)\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "dataset, _, x_test, y_test = load_real_samples()\n",
    "train(encoder, generator, discriminator, gan_model, latent_dim, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
