{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "syleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianChengEn/lab-project/blob/main/styleGAN/syleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I3MCigdudVD",
        "outputId": "16906cef-2384-4cf2-a222-5870d26eb65f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GmA6Uv7JHFCC",
        "outputId": "96b3ab33-428b-44be-df0d-d55163bbcd2d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow.keras.initializers as initer\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, Flatten, Dense, Reshape, Conv2DTranspose, ReLU, BatchNormalization, LeakyReLU\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# -------------------get_half_batch_ds-------------------\n",
        "\n",
        "def _process_x(x):\n",
        "    return tf.expand_dims(tf.cast(x, tf.float32), axis=3) / 255. * 2 - 1\n",
        "\n",
        "def get_half_batch_ds(batch_size):\n",
        "    return get_ds(batch_size//2)\n",
        "\n",
        "def get_ds(batch_size):\n",
        "    (x, y), _ = keras.datasets.mnist.load_data()\n",
        "    x = _process_x(x)\n",
        "    y = tf.cast(y, tf.int32)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x, y)).cache().shuffle(1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# -------------------------------------------------------\n",
        "\n",
        "def mnist_uni_disc_cnn(input_shape=(28, 28, 1), use_bn=True):\n",
        "    model = keras.Sequential()\n",
        "    # [n, 28, 28, n] -> [n, 14, 14, 64]\n",
        "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', input_shape=input_shape))\n",
        "    if use_bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "    # -> [n, 7, 7, 128]\n",
        "    model.add(Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    if use_bn:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Flatten())\n",
        "    return model\n",
        "\n",
        "# -------------------------------------------------------\n",
        "\n",
        "class AdaNorm(keras.layers.Layer):\n",
        "    def __init__(self, axis=(1, 2), epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        # NHWC\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        mean = tf.math.reduce_mean(x, axis=self.axis, keepdims=True)\n",
        "        diff = x - mean\n",
        "        variance = tf.reduce_mean(tf.math.square(diff), axis=self.axis, keepdims=True)\n",
        "        x_norm = diff * tf.math.rsqrt(variance + self.epsilon)\n",
        "        return x_norm\n",
        "\n",
        "\n",
        "class AdaMod(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.y = None\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x, w = inputs\n",
        "        y = self.y(w)\n",
        "        o = (y[:, 0] + 1) * x + y[:, 1]\n",
        "        return o\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        x_shape, w_shape = input_shape\n",
        "        self.y = keras.Sequential([\n",
        "            keras.layers.Dense(x_shape[-1]*2, input_shape=w_shape[1:], name=\"y\",\n",
        "                               kernel_initializer=initer.RandomNormal(0, 1)),   # this kernel is important\n",
        "            keras.layers.Reshape([2, 1, 1, -1])])  # [2, h, w, c]\n",
        "\n",
        "\n",
        "class AddNoise(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.s = None\n",
        "        self.x_shape = None\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x, noise = inputs\n",
        "        noise_ = noise[:, :self.x_shape[1], :self.x_shape[2], :]\n",
        "        return self.s * noise_ + x\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.x_shape, _ = input_shape\n",
        "        self.s = self.add_weight(name=\"noise_scale\", shape=[1, 1, self.x_shape[-1]],     # [h, w, c]\n",
        "                                 initializer=initer.random_normal(0., 1.))   # large initial noise\n",
        "\n",
        "\n",
        "class Map(keras.layers.Layer):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.f = None\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        w = self.f(inputs)\n",
        "        return w\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.f = keras.Sequential([\n",
        "            keras.layers.Dense(self.size, input_shape=input_shape[1:]),\n",
        "            # keras.layers.LeakyReLU(0.2),  # worse performance when using non-linearity in mapping\n",
        "            keras.layers.Dense(self.size),\n",
        "        ])\n",
        "\n",
        "\n",
        "class Style(keras.layers.Layer):\n",
        "    def __init__(self, filters, upsampling=True):\n",
        "        super().__init__()\n",
        "        self.filters = filters\n",
        "        self.upsampling = upsampling\n",
        "        self.ada_mod, self.ada_norm, self.add_noise, self.up, self.conv = None, None, None, None, None\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x, w, noise = inputs\n",
        "        x = self.ada_mod((x, w))\n",
        "        if self.up is not None:\n",
        "            x = self.up(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.ada_norm(x)\n",
        "        x = keras.layers.LeakyReLU()(x)\n",
        "        x = self.add_noise((x, noise))\n",
        "        return x\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.ada_mod = AdaMod()\n",
        "        self.ada_norm = AdaNorm()\n",
        "        if self.upsampling:\n",
        "            self.up = keras.layers.UpSampling2D((2, 2), interpolation=\"bilinear\")\n",
        "        self.add_noise = AddNoise()\n",
        "        self.conv = keras.layers.Conv2D(self.filters, 3, 1, \"same\")\n",
        "\n",
        "\n",
        "class StyleGAN(keras.Model):\n",
        "    \"\"\"\n",
        "    重新定义generator,生成图片\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim, img_shape):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_shape = img_shape\n",
        "        self.n_style = 3\n",
        "\n",
        "        self.g = self._get_generator()\n",
        "        self.d = self._get_discriminator()\n",
        "\n",
        "        self.opt = keras.optimizers.Adam(0.001, beta_1=0.)\n",
        "        self.loss_bool = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        if isinstance(inputs[0], np.ndarray):\n",
        "            inputs = (tf.convert_to_tensor(i) for i in inputs)\n",
        "        inputs = [tf.ones((len(inputs[0]), 1)), *inputs]\n",
        "        return self.g.call(inputs, training=training)\n",
        "\n",
        "    def _get_generator(self):\n",
        "        z = keras.Input((self.n_style, self.latent_dim,), name=\"z\")\n",
        "        noise_ = keras.Input((self.img_shape[0], self.img_shape[1]), name=\"noise\")\n",
        "        ones = keras.Input((1,), name=\"ones\")\n",
        "\n",
        "        const = keras.Sequential([\n",
        "            keras.layers.Dense(7*7*128, use_bias=False, name=\"const\"),\n",
        "            keras.layers.Reshape((7, 7, 128)),\n",
        "        ], name=\"const\")(ones)\n",
        "\n",
        "        w = Map(size=128)(z)\n",
        "        print(w.shape)\n",
        "        print(z.shape)\n",
        "        print(w[:,0].shape)\n",
        "        noise = tf.expand_dims(noise_, axis=-1)\n",
        "        x = AddNoise()((const, noise))\n",
        "        x = AdaNorm()(x)\n",
        "        x = Style(64, upsampling=False)((x, w[:, 0], noise))    # 7^2\n",
        "        x = Style(64)((x, w[:, 1], noise))      # 14^2\n",
        "        x = Style(64)((x, w[:, 2], noise))  # 28^2\n",
        "        o = keras.layers.Conv2D(self.img_shape[-1], 5, 1, \"same\", activation=keras.activations.tanh)(x)\n",
        "\n",
        "        g = keras.Model([ones, z, noise_], o, name=\"generator\")\n",
        "        g.summary()\n",
        "        return g\n",
        "\n",
        "    def _get_discriminator(self):\n",
        "        model = keras.Sequential([\n",
        "            mnist_uni_disc_cnn(self.img_shape, use_bn=True),\n",
        "            keras.layers.Dense(1)\n",
        "        ], name=\"discriminator\")\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def train_d(self, img, label):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = self.d.call(img, training=True)\n",
        "            loss = self.loss_bool(label, pred)\n",
        "        grads = tape.gradient(loss, self.d.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.d.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def train_g(self, n):\n",
        "        available_z = [tf.random.normal((n, 1, self.latent_dim)) for _ in range(2)]\n",
        "        z = tf.concat([available_z[np.random.randint(0, len(available_z))] for _ in range(self.n_style)], axis=1)\n",
        "\n",
        "        noise = tf.random.normal((n, self.img_shape[0], self.img_shape[1]))\n",
        "        inputs = (z, noise)\n",
        "        with tf.GradientTape() as tape:\n",
        "            g_img = self.call(inputs, training=True)\n",
        "            pred = self.d.call(g_img, training=False)\n",
        "            loss = self.loss_bool(tf.ones_like(pred), pred)\n",
        "        grads = tape.gradient(loss, self.g.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.g.trainable_variables))\n",
        "        return loss, g_img\n",
        "\n",
        "    def step(self, img):\n",
        "        g_loss, g_img = self.train_g(len(img) * 2)\n",
        "        d_label = tf.concat((tf.ones((len(img), 1), tf.float32), tf.zeros((len(g_img) // 2, 1), tf.float32)), axis=0)\n",
        "        img = tf.concat((img, g_img[:len(g_img) // 2]), axis=0)\n",
        "        d_loss = self.train_d(img, d_label)\n",
        "        return d_loss, g_loss, g_img\n",
        "\n",
        "\n",
        "def train(gan, ds, epoch):\n",
        "    t0 = time.time()\n",
        "    for ep in range(epoch):\n",
        "        for t, (img, _) in enumerate(ds):\n",
        "            d_loss, g_loss, g_img = gan.step(img)\n",
        "            if t % 400 == 0:\n",
        "                t1 = time.time()\n",
        "                print(\n",
        "                    \"ep={} | time={:.1f} | t={} | d_loss={:.2f} | g_loss={:.2f}\".format(\n",
        "                        ep, t1 - t0, t, d_loss.numpy(), g_loss.numpy(), ))\n",
        "                t0 = t1\n",
        "        save_gan(gan, ep)\n",
        "\n",
        "def save_gan(model, a, **kwargs):\n",
        "    n = 12\n",
        "    global z1, z2       # z1 row, z2 col\n",
        "    if \"z1\" not in globals():\n",
        "        z1 = np.random.normal(0, 1, size=(n, 1, model.latent_dim))\n",
        "    if \"z2\" not in globals():\n",
        "        z2 = np.random.normal(0, 1, size=(n, 1, model.latent_dim))\n",
        "    imgs = model.predict([\n",
        "        np.concatenate(\n",
        "            (z1.repeat(n, axis=0).repeat(1, axis=1), np.repeat(np.concatenate([z2 for _ in range(n)], axis=0), 2, axis=1)),\n",
        "            axis=1),\n",
        "        np.zeros([len(z1)*n, model.img_shape[0], model.img_shape[1]], dtype=np.float32)])\n",
        "    z1_imgs = -model.predict([z1.repeat(model.n_style, axis=1), np.zeros([len(z1), model.img_shape[0], model.img_shape[1]], dtype=np.float32)])\n",
        "    z2_imgs = -model.predict([z2.repeat(model.n_style, axis=1), np.zeros([len(z2), model.img_shape[0], model.img_shape[1]], dtype=np.float32)])\n",
        "    imgs = np.concatenate([z2_imgs, imgs], axis=0)\n",
        "    rest_imgs = np.concatenate([np.ones([1, 28, 28, 1], dtype=np.float32), z1_imgs], axis=0)\n",
        "    for i in range(len(rest_imgs)):\n",
        "        imgs = np.concatenate([imgs[:i*(n+1)], rest_imgs[i:i+1], imgs[i*(n+1):]], axis=0)\n",
        "    _save_gan(imgs, a, show_label=False, nc=n+1, nr=n+1)\n",
        "\n",
        "def _save_gan(imgs, a, show_label=False, nc=5, nr=5):\n",
        "    if not isinstance(imgs, np.ndarray):\n",
        "        imgs = imgs.numpy()\n",
        "    if imgs.ndim > 3:\n",
        "        imgs = np.squeeze(imgs, axis=-1)\n",
        "    plt.clf()\n",
        "    plt.figure(0, (nc * 2, nr * 2))\n",
        "    for c in range(nc):\n",
        "        for r in range(nr):\n",
        "            i = r * nc + c\n",
        "            plt.subplot(nr, nc, i + 1)\n",
        "            plt.imshow(imgs[i], cmap=\"gray_r\")\n",
        "            plt.axis(\"off\")\n",
        "    plt.savefig('/content/drive/MyDrive/張雲南/styleGAN/img/' + f'{a}_{imgs[0].shape}.png')\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    LATENT_DIM = 100\n",
        "    IMG_SHAPE = (28, 28, 1)\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCH = 20\n",
        "\n",
        "    d = get_half_batch_ds(BATCH_SIZE)\n",
        "    m = StyleGAN(LATENT_DIM, IMG_SHAPE)\n",
        "    train(m, d, EPOCH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 3, 128)\n",
            "(None, 3, 100)\n",
            "(None, 128)\n",
            "Model: \"generator\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " ones (InputLayer)              [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " noise (InputLayer)             [(None, 28, 28)]     0           []                               \n",
            "                                                                                                  \n",
            " const (Sequential)             (None, 7, 7, 128)    6272        ['ones[0][0]']                   \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda)    (None, 28, 28, 1)    0           ['noise[0][0]']                  \n",
            "                                                                                                  \n",
            " z (InputLayer)                 [(None, 3, 100)]     0           []                               \n",
            "                                                                                                  \n",
            " add_noise (AddNoise)           (None, 7, 7, 128)    128         ['const[0][0]',                  \n",
            "                                                                  'tf.expand_dims[0][0]']         \n",
            "                                                                                                  \n",
            " map (Map)                      (None, 3, 128)       29440       ['z[0][0]']                      \n",
            "                                                                                                  \n",
            " ada_norm (AdaNorm)             (None, 7, 7, 128)    0           ['add_noise[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 128)         0           ['map[0][0]']                    \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " style (Style)                  (None, 7, 7, 64)     106880      ['ada_norm[0][0]',               \n",
            "                                                                  'tf.__operators__.getitem_1[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'tf.expand_dims[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 128)         0           ['map[0][0]']                    \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " style_1 (Style)                (None, 14, 14, 64)   53504       ['style[0][0]',                  \n",
            "                                                                  'tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'tf.expand_dims[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3 (Sl  (None, 128)         0           ['map[0][0]']                    \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " style_2 (Style)                (None, 28, 28, 64)   53504       ['style_1[0][0]',                \n",
            "                                                                  'tf.__operators__.getitem_3[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'tf.expand_dims[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 28, 28, 1)    1601        ['style_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 251,329\n",
            "Trainable params: 251,329\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 6272)              133056    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 6273      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139,329\n",
            "Trainable params: 138,945\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "ep=0 | time=3.7 | t=0 | d_loss=0.71 | g_loss=0.71\n",
            "ep=0 | time=54.7 | t=400 | d_loss=0.52 | g_loss=0.73\n",
            "ep=0 | time=47.8 | t=800 | d_loss=0.54 | g_loss=0.86\n",
            "ep=0 | time=48.6 | t=1200 | d_loss=0.58 | g_loss=0.92\n",
            "ep=0 | time=47.6 | t=1600 | d_loss=0.55 | g_loss=1.57\n",
            "ep=1 | time=39.6 | t=0 | d_loss=0.60 | g_loss=0.92\n",
            "ep=1 | time=49.0 | t=400 | d_loss=0.63 | g_loss=0.77\n",
            "ep=1 | time=48.4 | t=800 | d_loss=0.52 | g_loss=1.17\n",
            "ep=1 | time=48.3 | t=1200 | d_loss=0.65 | g_loss=1.79\n",
            "ep=1 | time=47.7 | t=1600 | d_loss=0.58 | g_loss=1.04\n",
            "ep=2 | time=37.5 | t=0 | d_loss=0.49 | g_loss=1.24\n",
            "ep=2 | time=47.3 | t=400 | d_loss=0.66 | g_loss=0.79\n",
            "ep=2 | time=48.0 | t=800 | d_loss=0.48 | g_loss=1.11\n",
            "ep=2 | time=47.9 | t=1200 | d_loss=0.65 | g_loss=1.39\n",
            "ep=2 | time=47.5 | t=1600 | d_loss=0.60 | g_loss=0.99\n",
            "ep=3 | time=37.7 | t=0 | d_loss=0.34 | g_loss=1.42\n",
            "ep=3 | time=48.0 | t=400 | d_loss=0.57 | g_loss=0.64\n",
            "ep=3 | time=47.3 | t=800 | d_loss=0.63 | g_loss=0.84\n",
            "ep=3 | time=47.3 | t=1200 | d_loss=0.61 | g_loss=0.86\n",
            "ep=3 | time=46.9 | t=1600 | d_loss=0.70 | g_loss=0.43\n",
            "ep=4 | time=37.1 | t=0 | d_loss=0.68 | g_loss=0.88\n",
            "ep=4 | time=47.1 | t=400 | d_loss=0.68 | g_loss=0.59\n",
            "ep=4 | time=46.9 | t=800 | d_loss=0.63 | g_loss=1.31\n",
            "ep=4 | time=47.8 | t=1200 | d_loss=0.59 | g_loss=1.15\n",
            "ep=4 | time=47.0 | t=1600 | d_loss=0.60 | g_loss=0.74\n",
            "ep=5 | time=37.8 | t=0 | d_loss=0.57 | g_loss=0.88\n",
            "ep=5 | time=47.1 | t=400 | d_loss=0.57 | g_loss=0.91\n",
            "ep=5 | time=46.9 | t=800 | d_loss=0.63 | g_loss=0.63\n",
            "ep=5 | time=47.5 | t=1200 | d_loss=0.59 | g_loss=0.66\n",
            "ep=5 | time=48.6 | t=1600 | d_loss=0.58 | g_loss=0.81\n",
            "ep=6 | time=37.4 | t=0 | d_loss=0.73 | g_loss=0.59\n",
            "ep=6 | time=47.6 | t=400 | d_loss=0.64 | g_loss=1.03\n",
            "ep=6 | time=47.2 | t=800 | d_loss=0.58 | g_loss=0.57\n",
            "ep=6 | time=47.7 | t=1200 | d_loss=0.49 | g_loss=0.67\n",
            "ep=6 | time=47.8 | t=1600 | d_loss=0.61 | g_loss=0.99\n",
            "ep=7 | time=37.7 | t=0 | d_loss=0.51 | g_loss=1.42\n",
            "ep=7 | time=46.9 | t=400 | d_loss=0.61 | g_loss=0.93\n",
            "ep=7 | time=47.2 | t=800 | d_loss=0.55 | g_loss=1.15\n",
            "ep=7 | time=47.6 | t=1200 | d_loss=0.61 | g_loss=0.93\n",
            "ep=7 | time=46.8 | t=1600 | d_loss=0.55 | g_loss=1.46\n",
            "ep=8 | time=37.9 | t=0 | d_loss=0.63 | g_loss=1.08\n",
            "ep=8 | time=46.8 | t=400 | d_loss=0.47 | g_loss=1.21\n",
            "ep=8 | time=48.4 | t=800 | d_loss=0.54 | g_loss=1.23\n",
            "ep=8 | time=47.6 | t=1200 | d_loss=0.65 | g_loss=1.53\n",
            "ep=8 | time=47.4 | t=1600 | d_loss=0.52 | g_loss=0.84\n",
            "ep=9 | time=36.7 | t=0 | d_loss=0.55 | g_loss=1.42\n",
            "ep=9 | time=46.8 | t=400 | d_loss=0.65 | g_loss=1.41\n",
            "ep=9 | time=46.8 | t=800 | d_loss=0.64 | g_loss=0.65\n",
            "ep=9 | time=46.8 | t=1200 | d_loss=0.53 | g_loss=0.94\n",
            "ep=9 | time=46.6 | t=1600 | d_loss=0.57 | g_loss=0.73\n",
            "ep=10 | time=36.7 | t=0 | d_loss=0.58 | g_loss=0.90\n",
            "ep=10 | time=47.7 | t=400 | d_loss=0.58 | g_loss=1.34\n",
            "ep=10 | time=47.4 | t=800 | d_loss=0.50 | g_loss=1.18\n",
            "ep=10 | time=47.2 | t=1200 | d_loss=0.67 | g_loss=0.81\n",
            "ep=10 | time=47.3 | t=1600 | d_loss=0.54 | g_loss=1.03\n",
            "ep=11 | time=38.5 | t=0 | d_loss=0.61 | g_loss=0.64\n",
            "ep=11 | time=47.2 | t=400 | d_loss=0.47 | g_loss=0.77\n",
            "ep=11 | time=48.1 | t=800 | d_loss=0.53 | g_loss=0.87\n",
            "ep=11 | time=47.3 | t=1200 | d_loss=0.65 | g_loss=0.84\n",
            "ep=11 | time=46.5 | t=1600 | d_loss=0.71 | g_loss=1.82\n",
            "ep=12 | time=36.8 | t=0 | d_loss=0.64 | g_loss=0.62\n",
            "ep=12 | time=46.8 | t=400 | d_loss=0.54 | g_loss=1.63\n",
            "ep=12 | time=47.2 | t=800 | d_loss=0.57 | g_loss=0.74\n",
            "ep=12 | time=47.0 | t=1200 | d_loss=0.67 | g_loss=1.36\n",
            "ep=12 | time=46.6 | t=1600 | d_loss=0.63 | g_loss=0.69\n",
            "ep=13 | time=36.8 | t=0 | d_loss=0.56 | g_loss=1.14\n",
            "ep=13 | time=47.2 | t=400 | d_loss=0.56 | g_loss=1.03\n",
            "ep=13 | time=47.5 | t=800 | d_loss=0.60 | g_loss=1.43\n",
            "ep=13 | time=46.6 | t=1200 | d_loss=0.76 | g_loss=0.42\n",
            "ep=13 | time=47.0 | t=1600 | d_loss=0.57 | g_loss=0.74\n",
            "ep=14 | time=38.4 | t=0 | d_loss=0.58 | g_loss=1.47\n",
            "ep=14 | time=47.2 | t=400 | d_loss=0.48 | g_loss=1.03\n",
            "ep=14 | time=47.0 | t=800 | d_loss=0.47 | g_loss=0.87\n",
            "ep=14 | time=46.8 | t=1200 | d_loss=0.46 | g_loss=0.93\n",
            "ep=14 | time=46.6 | t=1600 | d_loss=0.60 | g_loss=1.03\n",
            "ep=15 | time=37.1 | t=0 | d_loss=0.62 | g_loss=0.71\n",
            "ep=15 | time=47.4 | t=400 | d_loss=0.71 | g_loss=0.49\n",
            "ep=15 | time=46.6 | t=800 | d_loss=0.59 | g_loss=0.72\n",
            "ep=15 | time=46.4 | t=1200 | d_loss=0.57 | g_loss=1.06\n",
            "ep=15 | time=46.5 | t=1600 | d_loss=0.56 | g_loss=1.12\n",
            "ep=16 | time=36.5 | t=0 | d_loss=0.74 | g_loss=0.50\n",
            "ep=16 | time=46.6 | t=400 | d_loss=0.54 | g_loss=1.44\n",
            "ep=16 | time=46.6 | t=800 | d_loss=0.56 | g_loss=1.04\n",
            "ep=16 | time=46.8 | t=1200 | d_loss=0.68 | g_loss=1.20\n",
            "ep=16 | time=46.8 | t=1600 | d_loss=0.86 | g_loss=1.92\n",
            "ep=17 | time=38.8 | t=0 | d_loss=0.51 | g_loss=1.31\n",
            "ep=17 | time=47.7 | t=400 | d_loss=0.64 | g_loss=0.92\n",
            "ep=17 | time=46.3 | t=800 | d_loss=0.61 | g_loss=1.70\n",
            "ep=17 | time=46.7 | t=1200 | d_loss=0.79 | g_loss=0.41\n",
            "ep=17 | time=46.4 | t=1600 | d_loss=0.58 | g_loss=0.83\n",
            "ep=18 | time=36.7 | t=0 | d_loss=0.59 | g_loss=1.53\n",
            "ep=18 | time=47.9 | t=400 | d_loss=0.56 | g_loss=0.89\n",
            "ep=18 | time=48.1 | t=800 | d_loss=0.43 | g_loss=1.23\n",
            "ep=18 | time=46.6 | t=1200 | d_loss=0.56 | g_loss=0.94\n",
            "ep=18 | time=46.4 | t=1600 | d_loss=0.67 | g_loss=1.29\n",
            "ep=19 | time=36.8 | t=0 | d_loss=0.57 | g_loss=0.90\n",
            "ep=19 | time=46.7 | t=400 | d_loss=0.48 | g_loss=1.32\n",
            "ep=19 | time=46.8 | t=800 | d_loss=0.52 | g_loss=1.23\n",
            "ep=19 | time=47.3 | t=1200 | d_loss=0.65 | g_loss=0.55\n",
            "ep=19 | time=46.7 | t=1600 | d_loss=0.75 | g_loss=1.61\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}